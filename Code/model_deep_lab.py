# -*- coding: utf-8 -*-
"""Model_Deep_Lab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JkCPbcuzA2TQUS1Hbo8rjnxP1cJ9hSv1

**Importações**
"""

# Commented out IPython magic to ensure Python compatibility.
import cv2
import os
import json
import pandas as pd
import numpy as np
import random
from glob import glob
from scipy.io import loadmat
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
from PIL import Image
# %matplotlib inline

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import load_model

import warnings
warnings.filterwarnings("ignore")

"""**Pré-processamento**"""

with open(r'/content/drive/MyDrive/Dataset de roupas/label_descriptions.json') as labels_json:
    labels_df = json.load(labels_json)

print(labels_df['attributes'])

categories=pd.DataFrame(labels_df['categories'])
categories['supercategory'].value_counts()

attributes=pd.DataFrame(labels_df['attributes'])
attributes['supercategory'].value_counts()

train_df=pd.read_csv(r'/content/drive/MyDrive/Dataset de roupas/train.csv')
train_df['category']=train_df['ClassId'].str.split('_').str[0]
train_df['attributes']=train_df['ClassId'].str.split('_').str[1:]

co=[]
for i in range (len(train_df)):
    if len(train_df['attributes'].iloc[i])>0:
        co.append(len(train_df['attributes'].iloc[i]))
    else:
        co.append(0)
train_df['count']=co

def rle2mask(image_filename,mask_rle,cat,H,W):
   mask=np.ones((H*W),dtype='uint8')*255

   en_list=mask_rle

   cat = np.int32(cat)
   sorted_index = np.argsort(cat)
   
   for i in sorted_index:


     px_list=list(map(int,en_list[i].split(" ")[0::2]))
     
    
     px_count_list=list(map(int,en_list[i].split(" ")[1::2]))
     
    
     cat_list=int(cat[i])
     
     tu=list(map(lambda x, y:(x,y), px_list, px_count_list))
     for p,m in tu:
       
        mask[p:p+m]=cat_list
       
   
   mask = mask.reshape((W,H)).T
   mask=cv2.cvtColor(mask,cv2.COLOR_GRAY2BGR)
   mask=cv2.resize(mask, (512, 512),interpolation=cv2.INTER_NEAREST)
  
   return mask

image_df = train_df.groupby('ImageId')['EncodedPixels', 'category'].agg(lambda x: list(x))
size_df = train_df.groupby('ImageId')['Height', 'Width'].mean()
image_df = image_df.join(size_df, on='ImageId')

print("Total images: ", len(image_df))
image_df.head()

image_df2=image_df.reset_index()
image_df2.head()

tr=train_df.groupby('category')['ImageId'].agg(lambda x: list(x)).to_frame('imageid').reset_index()
tr['category']=tr['category'].astype('int')
tr1=tr.sort_values('category', ascending=True)
tr1 = tr1.reset_index(drop=True)
my_dict = {k:v for k,v in zip(tr1['category'], tr1.drop(columns='category').values)}

from sklearn.model_selection import train_test_split
X_train, X_test = train_test_split(image_df2, test_size = 0.2,random_state=42)
X_train, X_val = train_test_split(X_train, test_size = 0.2,random_state=42)
print('train shape : ',X_train.shape)
print('test shape : ', X_test.shape)
print('val shape : ', X_val.shape)

def parse_image(i,m):
    image = tf.io.read_file(r'/content/drive/MyDrive/Dataset de roupas/train/'+i)
    image = tf.image.decode_jpeg(image,channels=3)
   
    image = tf.image.resize(image, [256, 256])
    image = image / 255
    
    
    mask = tf.io.read_file(r'/content/drive/MyDrive/Mascara/'+m)
    
    mask = tf.image.decode_png(mask,channels=3)
    
    mask=tf.image.rgb_to_grayscale(mask)
    mask=tf.image.resize(mask,[256,256],method='nearest')
    
    
    mask=tf.where(condition=tf.math.equal(mask, 255), x=tf.cast(46, dtype=mask.dtype), y=mask)
    mask=tf.cast(mask,'float32')
    
    return image,mask

def random_augmentation(img, mask):
    if tf.random.uniform(()) > 0.5:
        img = tf.image.flip_left_right(img)
        mask = tf.image.flip_left_right(mask)
    

    return img, mask

def tfdata_train(Tr,batch_size=5):
      f = Tr["ImageId"]
      g = Tr["ImageId"].str.replace('.jpg','.png',regex=False)
      
      train_dataset=tf.data.Dataset.from_tensor_slices((f,g))
      train_dataset= train_dataset.shuffle(len(f))
      train_dataset= train_dataset.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
      train_dataset = train_dataset.map(random_augmentation, num_parallel_calls=tf.data.experimental.AUTOTUNE)
      train_dataset = train_dataset.batch(batch_size)
      train_dataset = train_dataset.prefetch(buffer_size=3)  
      return train_dataset

def tfdata_val(Val,batch_size=5):
    f = Val["ImageId"]
    g = Val["ImageId"].str.replace('.jpg','.png',regex=False)
    val_dataset = tf.data.Dataset.from_tensor_slices((f,g))
    val_dataset = val_dataset.shuffle(len(f))
    val_dataset = val_dataset.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    val_dataset = val_dataset.batch(batch_size)
    
    return val_dataset

def tfdata_test(Test,batch_size=5):
  f = Test["ImageId"]
  g = Test["ImageId"].str.replace('.jpg','.png',regex=False)
  test_dataset = tf.data.Dataset.from_tensor_slices((f, g))
  test_dataset = test_dataset.map(parse_image)
  test_dataset = test_dataset.batch(batch_size)
  
  return test_dataset

train_dataset=tfdata_train(X_train)
val_dataset=tfdata_val(X_val)
test_dataset =tfdata_test(X_test)

IMAGE_SIZE = 256
BATCH_SIZE = 200
NUM_CLASSES = 47

"""**Modelo**"""

def convolution_block(
    block_input,
    num_filters=16,
    kernel_size=3,
    dilation_rate=1,
    padding="same",
    use_bias=False,
):
    x = layers.Conv2D(
        num_filters,
        kernel_size=kernel_size,
        dilation_rate=dilation_rate,
        padding="same",
        use_bias=use_bias,
        kernel_initializer=keras.initializers.HeNormal(),
    )(block_input)
    x = layers.BatchNormalization()(x)
    return tf.nn.relu(x)


def DilatedSpatialPyramidPooling(dspp_input):
    dims = dspp_input.shape
    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)
    x = convolution_block(x, kernel_size=1, use_bias=True)
    out_pool = layers.UpSampling2D(
        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation="bilinear",
    )(x)

    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)
    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)
    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)
    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)

    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])
    output = convolution_block(x, kernel_size=1)
    return output

def DeeplabV3Plus(image_size, num_classes):
    model_input = keras.Input(shape=(image_size, image_size, 3))
    resnet50 = keras.applications.ResNet50(
        weights="imagenet", include_top=False, input_tensor=model_input
    )
    x = resnet50.get_layer("conv4_block6_2_relu").output
    x = DilatedSpatialPyramidPooling(x)

    input_a = layers.UpSampling2D(
        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),
        interpolation="bilinear",
    )(x)
    input_b = resnet50.get_layer("conv2_block3_2_relu").output
    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)

    x = layers.Concatenate(axis=-1)([input_a, input_b])
    x = convolution_block(x)
    x = convolution_block(x)
    x = layers.UpSampling2D(
        size=(image_size // x.shape[1], image_size // x.shape[2]),
        interpolation="bilinear",
    )(x)
    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding="same")(x)
    return keras.Model(inputs=model_input, outputs=model_output)


model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)
model.summary()

"""**Treinamento**"""

STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE
VALIDATION_STEPS = X_val.shape[0] // BATCH_SIZE

tensorboard_callback = tf.keras.callbacks.TensorBoard('./logs', histogram_freq=1)
callbacks = [

    tensorboard_callback,
    tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),
    tf.keras.callbacks.ModelCheckpoint('pesos.h5', verbose=1, save_weights_only=True)
     ]

loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.01),
    loss=loss,
    metrics=["accuracy"],
)

history = model.fit(train_dataset, validation_data=val_dataset, 
                    epochs=6,steps_per_epoch=STEPS_PER_EPOCH,
                    batch_size=BATCH_SIZE,
                    callbacks=callbacks)

model.save('deep_lab_final.h5')